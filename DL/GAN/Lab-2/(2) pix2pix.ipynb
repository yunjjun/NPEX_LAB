{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(2) pix2pix.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPv61kiFKqN8z24NIGujdEz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"szfSTP2MkCZs","colab_type":"text"},"source":["# Image-to-Image Translation : *pix2pix*"]},{"cell_type":"markdown","metadata":{"id":"DDiPyxsT-_ri","colab_type":"text"},"source":["## Download scripts & datasets"]},{"cell_type":"code","metadata":{"id":"DyhwLuqZQAhk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597339880019,"user_tz":-540,"elapsed":40251,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}},"outputId":"7a0d1a99-fb97-4ad5-ac10-2a56965cbffa"},"source":["# !git clone https://github.com/whieya/NPEX/GAN_projects\n","# !bash ./datasets/download_pix2pix_dataset.sh edges2handbags\n","\n","# download dataset\n","!mkdir ./datasets\n","!wget -N http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz -O ./datasets/facades.tar.gz\n","!mkdir -p ./datasets/facades/\n","!tar -zxvf ./datasets/facades.tar.gz -C ./datasets/\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING: timestamping does nothing in combination with -O. See the manual\n","for details.\n","\n","--2020-08-13 17:30:27--  http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n","Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.189.73\n","Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.189.73|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 30168306 (29M) [application/x-gzip]\n","Saving to: ‘./datasets/facades.tar.gz’\n","\n","./datasets/facades. 100%[===================>]  28.77M  1.64MB/s    in 30s     \n","\n","2020-08-13 17:30:58 (968 KB/s) - ‘./datasets/facades.tar.gz’ saved [30168306/30168306]\n","\n","facades/\n","facades/test/\n","facades/test/27.jpg\n","facades/test/5.jpg\n","facades/test/72.jpg\n","facades/test/1.jpg\n","facades/test/10.jpg\n","facades/test/100.jpg\n","facades/test/101.jpg\n","facades/test/102.jpg\n","facades/test/103.jpg\n","facades/test/104.jpg\n","facades/test/105.jpg\n","facades/test/106.jpg\n","facades/test/11.jpg\n","facades/test/12.jpg\n","facades/test/13.jpg\n","facades/test/14.jpg\n","facades/test/15.jpg\n","facades/test/16.jpg\n","facades/test/17.jpg\n","facades/test/18.jpg\n","facades/test/19.jpg\n","facades/test/2.jpg\n","facades/test/20.jpg\n","facades/test/21.jpg\n","facades/test/22.jpg\n","facades/test/23.jpg\n","facades/test/24.jpg\n","facades/test/25.jpg\n","facades/test/26.jpg\n","facades/test/50.jpg\n","facades/test/51.jpg\n","facades/test/52.jpg\n","facades/test/53.jpg\n","facades/test/54.jpg\n","facades/test/55.jpg\n","facades/test/56.jpg\n","facades/test/57.jpg\n","facades/test/58.jpg\n","facades/test/59.jpg\n","facades/test/6.jpg\n","facades/test/60.jpg\n","facades/test/61.jpg\n","facades/test/62.jpg\n","facades/test/63.jpg\n","facades/test/64.jpg\n","facades/test/65.jpg\n","facades/test/66.jpg\n","facades/test/67.jpg\n","facades/test/68.jpg\n","facades/test/69.jpg\n","facades/test/7.jpg\n","facades/test/70.jpg\n","facades/test/71.jpg\n","facades/test/73.jpg\n","facades/test/74.jpg\n","facades/test/75.jpg\n","facades/test/76.jpg\n","facades/test/77.jpg\n","facades/test/78.jpg\n","facades/test/79.jpg\n","facades/test/8.jpg\n","facades/test/80.jpg\n","facades/test/81.jpg\n","facades/test/82.jpg\n","facades/test/83.jpg\n","facades/test/84.jpg\n","facades/test/85.jpg\n","facades/test/86.jpg\n","facades/test/87.jpg\n","facades/test/88.jpg\n","facades/test/89.jpg\n","facades/test/9.jpg\n","facades/test/90.jpg\n","facades/test/91.jpg\n","facades/test/92.jpg\n","facades/test/93.jpg\n","facades/test/94.jpg\n","facades/test/95.jpg\n","facades/test/96.jpg\n","facades/test/97.jpg\n","facades/test/98.jpg\n","facades/test/99.jpg\n","facades/test/28.jpg\n","facades/test/29.jpg\n","facades/test/3.jpg\n","facades/test/30.jpg\n","facades/test/31.jpg\n","facades/test/32.jpg\n","facades/test/33.jpg\n","facades/test/34.jpg\n","facades/test/35.jpg\n","facades/test/36.jpg\n","facades/test/37.jpg\n","facades/test/38.jpg\n","facades/test/39.jpg\n","facades/test/4.jpg\n","facades/test/40.jpg\n","facades/test/41.jpg\n","facades/test/42.jpg\n","facades/test/43.jpg\n","facades/test/44.jpg\n","facades/test/45.jpg\n","facades/test/46.jpg\n","facades/test/47.jpg\n","facades/test/48.jpg\n","facades/test/49.jpg\n","facades/train/\n","facades/train/1.jpg\n","facades/train/10.jpg\n","facades/train/100.jpg\n","facades/train/101.jpg\n","facades/train/102.jpg\n","facades/train/103.jpg\n","facades/train/104.jpg\n","facades/train/105.jpg\n","facades/train/106.jpg\n","facades/train/107.jpg\n","facades/train/108.jpg\n","facades/train/109.jpg\n","facades/train/11.jpg\n","facades/train/110.jpg\n","facades/train/111.jpg\n","facades/train/112.jpg\n","facades/train/113.jpg\n","facades/train/114.jpg\n","facades/train/115.jpg\n","facades/train/116.jpg\n","facades/train/117.jpg\n","facades/train/118.jpg\n","facades/train/119.jpg\n","facades/train/12.jpg\n","facades/train/120.jpg\n","facades/train/121.jpg\n","facades/train/122.jpg\n","facades/train/123.jpg\n","facades/train/124.jpg\n","facades/train/125.jpg\n","facades/train/126.jpg\n","facades/train/309.jpg\n","facades/train/31.jpg\n","facades/train/310.jpg\n","facades/train/311.jpg\n","facades/train/312.jpg\n","facades/train/313.jpg\n","facades/train/314.jpg\n","facades/train/315.jpg\n","facades/train/316.jpg\n","facades/train/317.jpg\n","facades/train/318.jpg\n","facades/train/319.jpg\n","facades/train/32.jpg\n","facades/train/320.jpg\n","facades/train/321.jpg\n","facades/train/322.jpg\n","facades/train/323.jpg\n","facades/train/324.jpg\n","facades/train/325.jpg\n","facades/train/326.jpg\n","facades/train/327.jpg\n","facades/train/328.jpg\n","facades/train/329.jpg\n","facades/train/390.jpg\n","facades/train/391.jpg\n","facades/train/392.jpg\n","facades/train/393.jpg\n","facades/train/394.jpg\n","facades/train/395.jpg\n","facades/train/396.jpg\n","facades/train/397.jpg\n","facades/train/398.jpg\n","facades/train/399.jpg\n","facades/train/4.jpg\n","facades/train/40.jpg\n","facades/train/400.jpg\n","facades/train/41.jpg\n","facades/train/42.jpg\n","facades/train/43.jpg\n","facades/train/44.jpg\n","facades/train/45.jpg\n","facades/train/46.jpg\n","facades/train/47.jpg\n","facades/train/48.jpg\n","facades/train/49.jpg\n","facades/train/5.jpg\n","facades/train/50.jpg\n","facades/train/51.jpg\n","facades/train/52.jpg\n","facades/train/53.jpg\n","facades/train/54.jpg\n","facades/train/55.jpg\n","facades/train/56.jpg\n","facades/train/57.jpg\n","facades/train/58.jpg\n","facades/train/59.jpg\n","facades/train/6.jpg\n","facades/train/60.jpg\n","facades/train/61.jpg\n","facades/train/222.jpg\n","facades/train/223.jpg\n","facades/train/224.jpg\n","facades/train/225.jpg\n","facades/train/226.jpg\n","facades/train/227.jpg\n","facades/train/228.jpg\n","facades/train/229.jpg\n","facades/train/23.jpg\n","facades/train/230.jpg\n","facades/train/231.jpg\n","facades/train/232.jpg\n","facades/train/233.jpg\n","facades/train/234.jpg\n","facades/train/235.jpg\n","facades/train/236.jpg\n","facades/train/237.jpg\n","facades/train/238.jpg\n","facades/train/239.jpg\n","facades/train/24.jpg\n","facades/train/240.jpg\n","facades/train/241.jpg\n","facades/train/242.jpg\n","facades/train/243.jpg\n","facades/train/244.jpg\n","facades/train/245.jpg\n","facades/train/156.jpg\n","facades/train/157.jpg\n","facades/train/158.jpg\n","facades/train/159.jpg\n","facades/train/16.jpg\n","facades/train/160.jpg\n","facades/train/161.jpg\n","facades/train/162.jpg\n","facades/train/163.jpg\n","facades/train/164.jpg\n","facades/train/165.jpg\n","facades/train/166.jpg\n","facades/train/167.jpg\n","facades/train/168.jpg\n","facades/train/169.jpg\n","facades/train/17.jpg\n","facades/train/170.jpg\n","facades/train/171.jpg\n","facades/train/172.jpg\n","facades/train/173.jpg\n","facades/train/174.jpg\n","facades/train/175.jpg\n","facades/train/176.jpg\n","facades/train/177.jpg\n","facades/train/178.jpg\n","facades/train/179.jpg\n","facades/train/18.jpg\n","facades/train/180.jpg\n","facades/train/181.jpg\n","facades/train/182.jpg\n","facades/train/183.jpg\n","facades/train/184.jpg\n","facades/train/185.jpg\n","facades/train/186.jpg\n","facades/train/187.jpg\n","facades/train/188.jpg\n","facades/train/189.jpg\n","facades/train/19.jpg\n","facades/train/127.jpg\n","facades/train/155.jpg\n","facades/train/190.jpg\n","facades/train/221.jpg\n","facades/train/246.jpg\n","facades/train/27.jpg\n","facades/train/29.jpg\n","facades/train/308.jpg\n","facades/train/33.jpg\n","facades/train/350.jpg\n","facades/train/370.jpg\n","facades/train/39.jpg\n","facades/train/62.jpg\n","facades/train/270.jpg\n","facades/train/271.jpg\n","facades/train/272.jpg\n","facades/train/273.jpg\n","facades/train/274.jpg\n","facades/train/275.jpg\n","facades/train/276.jpg\n","facades/train/277.jpg\n","facades/train/278.jpg\n","facades/train/279.jpg\n","facades/train/28.jpg\n","facades/train/280.jpg\n","facades/train/281.jpg\n","facades/train/282.jpg\n","facades/train/283.jpg\n","facades/train/284.jpg\n","facades/train/285.jpg\n","facades/train/286.jpg\n","facades/train/287.jpg\n","facades/train/288.jpg\n","facades/train/289.jpg\n","facades/train/351.jpg\n","facades/train/352.jpg\n","facades/train/353.jpg\n","facades/train/354.jpg\n","facades/train/355.jpg\n","facades/train/356.jpg\n","facades/train/357.jpg\n","facades/train/358.jpg\n","facades/train/359.jpg\n","facades/train/36.jpg\n","facades/train/360.jpg\n","facades/train/361.jpg\n","facades/train/362.jpg\n","facades/train/363.jpg\n","facades/train/364.jpg\n","facades/train/365.jpg\n","facades/train/366.jpg\n","facades/train/367.jpg\n","facades/train/368.jpg\n","facades/train/369.jpg\n","facades/train/37.jpg\n","facades/train/63.jpg\n","facades/train/64.jpg\n","facades/train/65.jpg\n","facades/train/66.jpg\n","facades/train/67.jpg\n","facades/train/68.jpg\n","facades/train/69.jpg\n","facades/train/7.jpg\n","facades/train/70.jpg\n","facades/train/71.jpg\n","facades/train/72.jpg\n","facades/train/73.jpg\n","facades/train/74.jpg\n","facades/train/75.jpg\n","facades/train/76.jpg\n","facades/train/77.jpg\n","facades/train/78.jpg\n","facades/train/79.jpg\n","facades/train/8.jpg\n","facades/train/80.jpg\n","facades/train/81.jpg\n","facades/train/82.jpg\n","facades/train/83.jpg\n","facades/train/84.jpg\n","facades/train/85.jpg\n","facades/train/86.jpg\n","facades/train/87.jpg\n","facades/train/88.jpg\n","facades/train/89.jpg\n","facades/train/9.jpg\n","facades/train/90.jpg\n","facades/train/91.jpg\n","facades/train/92.jpg\n","facades/train/93.jpg\n","facades/train/94.jpg\n","facades/train/95.jpg\n","facades/train/96.jpg\n","facades/train/97.jpg\n","facades/train/98.jpg\n","facades/train/99.jpg\n","facades/train/128.jpg\n","facades/train/129.jpg\n","facades/train/13.jpg\n","facades/train/130.jpg\n","facades/train/131.jpg\n","facades/train/132.jpg\n","facades/train/133.jpg\n","facades/train/134.jpg\n","facades/train/135.jpg\n","facades/train/136.jpg\n","facades/train/137.jpg\n","facades/train/138.jpg\n","facades/train/139.jpg\n","facades/train/14.jpg\n","facades/train/140.jpg\n","facades/train/141.jpg\n","facades/train/142.jpg\n","facades/train/143.jpg\n","facades/train/144.jpg\n","facades/train/145.jpg\n","facades/train/146.jpg\n","facades/train/147.jpg\n","facades/train/148.jpg\n","facades/train/149.jpg\n","facades/train/15.jpg\n","facades/train/150.jpg\n","facades/train/151.jpg\n","facades/train/152.jpg\n","facades/train/153.jpg\n","facades/train/154.jpg\n","facades/train/191.jpg\n","facades/train/192.jpg\n","facades/train/193.jpg\n","facades/train/194.jpg\n","facades/train/195.jpg\n","facades/train/196.jpg\n","facades/train/197.jpg\n","facades/train/198.jpg\n","facades/train/199.jpg\n","facades/train/2.jpg\n","facades/train/20.jpg\n","facades/train/200.jpg\n","facades/train/201.jpg\n","facades/train/202.jpg\n","facades/train/203.jpg\n","facades/train/204.jpg\n","facades/train/205.jpg\n","facades/train/206.jpg\n","facades/train/207.jpg\n","facades/train/208.jpg\n","facades/train/209.jpg\n","facades/train/21.jpg\n","facades/train/210.jpg\n","facades/train/211.jpg\n","facades/train/212.jpg\n","facades/train/213.jpg\n","facades/train/214.jpg\n","facades/train/215.jpg\n","facades/train/216.jpg\n","facades/train/217.jpg\n","facades/train/218.jpg\n","facades/train/219.jpg\n","facades/train/22.jpg\n","facades/train/220.jpg\n","facades/train/247.jpg\n","facades/train/248.jpg\n","facades/train/249.jpg\n","facades/train/25.jpg\n","facades/train/250.jpg\n","facades/train/251.jpg\n","facades/train/252.jpg\n","facades/train/253.jpg\n","facades/train/254.jpg\n","facades/train/255.jpg\n","facades/train/256.jpg\n","facades/train/257.jpg\n","facades/train/258.jpg\n","facades/train/259.jpg\n","facades/train/26.jpg\n","facades/train/260.jpg\n","facades/train/261.jpg\n","facades/train/262.jpg\n","facades/train/263.jpg\n","facades/train/264.jpg\n","facades/train/265.jpg\n","facades/train/266.jpg\n","facades/train/267.jpg\n","facades/train/268.jpg\n","facades/train/269.jpg\n","facades/train/330.jpg\n","facades/train/331.jpg\n","facades/train/332.jpg\n","facades/train/333.jpg\n","facades/train/334.jpg\n","facades/train/335.jpg\n","facades/train/336.jpg\n","facades/train/337.jpg\n","facades/train/338.jpg\n","facades/train/339.jpg\n","facades/train/34.jpg\n","facades/train/340.jpg\n","facades/train/341.jpg\n","facades/train/342.jpg\n","facades/train/343.jpg\n","facades/train/344.jpg\n","facades/train/345.jpg\n","facades/train/346.jpg\n","facades/train/347.jpg\n","facades/train/348.jpg\n","facades/train/349.jpg\n","facades/train/35.jpg\n","facades/train/290.jpg\n","facades/train/291.jpg\n","facades/train/292.jpg\n","facades/train/293.jpg\n","facades/train/294.jpg\n","facades/train/295.jpg\n","facades/train/296.jpg\n","facades/train/297.jpg\n","facades/train/298.jpg\n","facades/train/299.jpg\n","facades/train/3.jpg\n","facades/train/30.jpg\n","facades/train/300.jpg\n","facades/train/301.jpg\n","facades/train/302.jpg\n","facades/train/303.jpg\n","facades/train/304.jpg\n","facades/train/305.jpg\n","facades/train/306.jpg\n","facades/train/307.jpg\n","facades/train/371.jpg\n","facades/train/372.jpg\n","facades/train/373.jpg\n","facades/train/374.jpg\n","facades/train/375.jpg\n","facades/train/376.jpg\n","facades/train/377.jpg\n","facades/train/378.jpg\n","facades/train/379.jpg\n","facades/train/38.jpg\n","facades/train/380.jpg\n","facades/train/381.jpg\n","facades/train/382.jpg\n","facades/train/383.jpg\n","facades/train/384.jpg\n","facades/train/385.jpg\n","facades/train/386.jpg\n","facades/train/387.jpg\n","facades/train/388.jpg\n","facades/train/389.jpg\n","facades/val/\n","facades/val/30.jpg\n","facades/val/50.jpg\n","facades/val/73.jpg\n","facades/val/1.jpg\n","facades/val/10.jpg\n","facades/val/100.jpg\n","facades/val/11.jpg\n","facades/val/12.jpg\n","facades/val/13.jpg\n","facades/val/14.jpg\n","facades/val/15.jpg\n","facades/val/16.jpg\n","facades/val/17.jpg\n","facades/val/18.jpg\n","facades/val/19.jpg\n","facades/val/2.jpg\n","facades/val/20.jpg\n","facades/val/21.jpg\n","facades/val/22.jpg\n","facades/val/23.jpg\n","facades/val/24.jpg\n","facades/val/25.jpg\n","facades/val/26.jpg\n","facades/val/27.jpg\n","facades/val/28.jpg\n","facades/val/29.jpg\n","facades/val/3.jpg\n","facades/val/51.jpg\n","facades/val/52.jpg\n","facades/val/53.jpg\n","facades/val/54.jpg\n","facades/val/55.jpg\n","facades/val/56.jpg\n","facades/val/57.jpg\n","facades/val/58.jpg\n","facades/val/59.jpg\n","facades/val/6.jpg\n","facades/val/60.jpg\n","facades/val/61.jpg\n","facades/val/62.jpg\n","facades/val/63.jpg\n","facades/val/64.jpg\n","facades/val/65.jpg\n","facades/val/66.jpg\n","facades/val/67.jpg\n","facades/val/68.jpg\n","facades/val/69.jpg\n","facades/val/7.jpg\n","facades/val/70.jpg\n","facades/val/71.jpg\n","facades/val/72.jpg\n","facades/val/74.jpg\n","facades/val/75.jpg\n","facades/val/76.jpg\n","facades/val/77.jpg\n","facades/val/78.jpg\n","facades/val/79.jpg\n","facades/val/8.jpg\n","facades/val/80.jpg\n","facades/val/81.jpg\n","facades/val/82.jpg\n","facades/val/83.jpg\n","facades/val/84.jpg\n","facades/val/85.jpg\n","facades/val/86.jpg\n","facades/val/87.jpg\n","facades/val/88.jpg\n","facades/val/89.jpg\n","facades/val/9.jpg\n","facades/val/90.jpg\n","facades/val/91.jpg\n","facades/val/92.jpg\n","facades/val/93.jpg\n","facades/val/94.jpg\n","facades/val/95.jpg\n","facades/val/96.jpg\n","facades/val/97.jpg\n","facades/val/98.jpg\n","facades/val/99.jpg\n","facades/val/31.jpg\n","facades/val/32.jpg\n","facades/val/33.jpg\n","facades/val/34.jpg\n","facades/val/35.jpg\n","facades/val/36.jpg\n","facades/val/37.jpg\n","facades/val/38.jpg\n","facades/val/39.jpg\n","facades/val/4.jpg\n","facades/val/40.jpg\n","facades/val/41.jpg\n","facades/val/42.jpg\n","facades/val/43.jpg\n","facades/val/44.jpg\n","facades/val/45.jpg\n","facades/val/46.jpg\n","facades/val/47.jpg\n","facades/val/48.jpg\n","facades/val/49.jpg\n","facades/val/5.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dJjSXzlAmCOI","colab_type":"text"},"source":["### Prepare DataLoader for MNIST dataset"]},{"cell_type":"code","metadata":{"id":"i5Fuxt5dP7IZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597339883911,"user_tz":-540,"elapsed":44120,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}}},"source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","# fix manual seed.\n","torch.manual_seed(1234)\n","\n","# set batch size.\n","BATCH_SIZE = 8\n","NUM_EXAMPLES = 8"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XsjfNBzc_id","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597339883918,"user_tz":-540,"elapsed":44114,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}}},"source":["import os\n","from torch.utils.data import Dataset\n","import PIL\n","\n","# Custom Dataset \n","class CustomFacades(Dataset):\n","    def __init__(self, root, train=True, transform=None):\n","        self.root = root\n","        self.transform = transform       \n","        self.train = train\n","        self.dir = 'train' if train else 'test'\n","        self.len_train = 400\n","        self.len_test = 106\n","\n","    def __len__(self):\n","        if self.train:\n","            return self.len_train\n","        else:\n","            return self.len_test\n","\n","    def __getitem__(self, idx):\n","        data_idx = idx if self.train else idx + self.len_train\n","        X = PIL.Image.open(os.path.join(self.root, self.dir, '{}.jpg'.format(idx+1)))\n","        if self.transform is not None:\n","            X = self.transform(X)\n","        return X[..., 256:], X[..., :256]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"uanjhhI5dIeH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597339883919,"user_tz":-540,"elapsed":44101,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}}},"source":["# prepare dataloader.\n","tf = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n","    ])\n","\n","# train loader for training GAN\n","train_dataset = CustomFacades(root='./datasets/facades', train=True, \n","        transform=tf)\n","train_loader = DataLoader(dataset=train_dataset, num_workers=8, batch_size=BATCH_SIZE, shuffle=True)\n","\n","# test loader for examining test samples\n","test_dataset = CustomFacades(root='./datasets/facades', train=False, transform=tf)\n","test_loader = DataLoader(dataset=test_dataset, num_workers=8, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yf6CYjavmK_y","colab_type":"text"},"source":["### Define GANs Models"]},{"cell_type":"markdown","metadata":{"id":"GFVC5C4wmQ5k","colab_type":"text"},"source":["#### Define Generator"]},{"cell_type":"code","metadata":{"id":"iuiBN6NTFDgN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597339883921,"user_tz":-540,"elapsed":44088,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}}},"source":["import torch.nn as nn\n","\n","# Unet Generator for pix2pix model.\n","# https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix \n","class UnetGenerator(nn.Module):\n","    \"\"\"Create a Unet-based generator\"\"\"\n","\n","    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n","        \"\"\"Construct a Unet generator\n","        Parameters:\n","            input_nc (int)  -- the number of channels in input images\n","            output_nc (int) -- the number of channels in output images\n","            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n","                                image of size 128x128 will become of size 1x1 # at the bottleneck\n","            ngf (int)       -- the number of filters in the last conv layer\n","            norm_layer      -- normalization layer\n","        We construct the U-Net from the innermost layer to the outermost layer.\n","        It is a recursive process.\n","        \"\"\"\n","        super(UnetGenerator, self).__init__()\n","        # construct unet structure\n","        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n","        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n","            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n","        # gradually reduce the number of filters from ngf * 8 to ngf\n","        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n","        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n","        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n","        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n","\n","    def forward(self, input):\n","        \"\"\"Standard forward\"\"\"\n","        return self.model(input)\n","\n","import functools\n","class UnetSkipConnectionBlock(nn.Module):\n","    \"\"\"Defines the Unet submodule with skip connection.\n","        X -------------------identity----------------------\n","        |-- downsampling -- |submodule| -- upsampling --|\n","    \"\"\"\n","\n","    def __init__(self, outer_nc, inner_nc, input_nc=None,\n","                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n","        \"\"\"Construct a Unet submodule with skip connections.\n","        Parameters:\n","            outer_nc (int) -- the number of filters in the outer conv layer\n","            inner_nc (int) -- the number of filters in the inner conv layer\n","            input_nc (int) -- the number of channels in input images/features\n","            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n","            outermost (bool)    -- if this module is the outermost module\n","            innermost (bool)    -- if this module is the innermost module\n","            norm_layer          -- normalization layer\n","            use_dropout (bool)  -- if use dropout layers.\n","        \"\"\"\n","        super(UnetSkipConnectionBlock, self).__init__()\n","        self.outermost = outermost\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","        if input_nc is None:\n","            input_nc = outer_nc\n","        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n","                             stride=2, padding=1, bias=use_bias)\n","        downrelu = nn.LeakyReLU(0.2, True)\n","        downnorm = norm_layer(inner_nc)\n","        uprelu = nn.ReLU(True)\n","        upnorm = norm_layer(outer_nc)\n","\n","        if outermost:\n","            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n","                                        kernel_size=4, stride=2,\n","                                        padding=1)\n","            down = [downconv]\n","            up = [uprelu, upconv, nn.Tanh()]\n","            model = down + [submodule] + up\n","        elif innermost:\n","            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n","                                        kernel_size=4, stride=2,\n","                                        padding=1, bias=use_bias)\n","            down = [downrelu, downconv]\n","            up = [uprelu, upconv, upnorm]\n","            model = down + up\n","        else:\n","            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n","                                        kernel_size=4, stride=2,\n","                                        padding=1, bias=use_bias)\n","            down = [downrelu, downconv, downnorm]\n","            up = [uprelu, upconv, upnorm]\n","\n","            if use_dropout:\n","                model = down + [submodule] + up + [nn.Dropout(0.5)]\n","            else:\n","                model = down + [submodule] + up\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        if self.outermost:\n","            return self.model(x)\n","        else:   # add skip connections\n","            return torch.cat([x, self.model(x)], 1)\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajYUrgp0mUt8","colab_type":"text"},"source":["#### Define Discriminator"]},{"cell_type":"code","metadata":{"id":"5upJT6CemOz6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597339883922,"user_tz":-540,"elapsed":44080,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}}},"source":["# Unet Discriminator for pix2pix model.\n","# https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix \n","class NLayerDiscriminator(nn.Module):\n","    \"\"\"Defines a PatchGAN discriminator\"\"\"\n","\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n","        \"\"\"Construct a PatchGAN discriminator\n","        Parameters:\n","            input_nc (int)  -- the number of channels in input images\n","            ndf (int)       -- the number of filters in the last conv layer\n","            n_layers (int)  -- the number of conv layers in the discriminator\n","            norm_layer      -- normalization layer\n","        \"\"\"\n","        super(NLayerDiscriminator, self).__init__()\n","        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        kw = 4\n","        padw = 1\n","        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n","        nf_mult = 1\n","        nf_mult_prev = 1\n","        for n in range(1, n_layers):  # gradually increase the number of filters\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2 ** n, 8)\n","            sequence += [\n","                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","                norm_layer(ndf * nf_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2 ** n_layers, 8)\n","        sequence += [\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n","        self.model = nn.Sequential(*sequence)\n","\n","    def forward(self, input):\n","        \"\"\"Standard forward.\"\"\"\n","        return self.model(input)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7yQUvj2rMLU","colab_type":"text"},"source":["#### Prepare GAN model and initialize the weights"]},{"cell_type":"code","metadata":{"id":"Irabx2XtoRhY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1597339892815,"user_tz":-540,"elapsed":52961,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}},"outputId":"46164650-19d5-4898-ff93-ad0c2a1f6ef5"},"source":["#weight initialization function. \n","def weights_init(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","            # m.weight.data.normal_(1.0, 0.2)\n","            nn.init.xavier_normal_(m.weight.data)\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","        elif isinstance(m, nn.BatchNorm2d):\n","            m.weight.data.normal_(1.0, 0.2)\n","            m.bias.data.zero_()\n","\n","# define GAN model.\n","G = UnetGenerator(input_nc=3, output_nc=3, num_downs=8).cuda()\n","D = NLayerDiscriminator(input_nc=3+3, ndf=64, n_layers=3).cuda()\n","\n","# weight initialization \n","G.apply(weights_init)\n","D.apply(weights_init)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NLayerDiscriminator(\n","  (model): Sequential(\n","    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"MhfKpgHmnQz_","colab_type":"text"},"source":["#### Start training GAN"]},{"cell_type":"code","metadata":{"id":"pJWTZUxxeQ2I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1597339900550,"user_tz":-540,"elapsed":60675,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}},"outputId":"5bafb9a1-b0c8-4c8e-fc2f-f173e773a411"},"source":["# install tensorboardx to use tensorboard.\n","%pip install tensorboardx\n","\n","from tensorboardX import SummaryWriter\n","from torchvision.utils import make_grid\n","import time\n","\n","# Hyper-parameters. \n","# ====== You don't need to change here ===== #\n","EPOCHS = 200\n","Z_DIM = 64\n","LAMBDA_L1 = 100\n","# ========================================== #\n","\n","# logger for tensorboard.\n","logger = SummaryWriter()\n","\n","# GT labels for calculating binary cross entropy loss. \n","real_label = torch.ones(size=(BATCH_SIZE,1)).cuda()\n","fake_label = torch.zeros(size=(BATCH_SIZE,1)).cuda()\n","\n","# criterion for binary cross entropy loss\n","BCE_criterion = torch.nn.BCELoss()\n","\n","# define optimizer. Here we use Adam optimizer. \n","optimizer_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5,0.999))\n","optimizer_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5,0.999))\n","\n","iterations = 0\n","\n","for epoch in range(EPOCHS):\n","    # Set both G&D train modes.\n","    G.train()\n","    D.train()\n","\n","    # For logging in tensorboard\n","    loss_G_total, loss_D_total = 0., 0.\n","    loss_G_sub_total, loss_D_sub_total = 0., 0.\n","\n","    for batch_idx, (real_A, real_B) in enumerate(train_loader):\n","        t1 = time.time()\n","        real_A = real_A.cuda()\n","        real_B = real_B.cuda()\n","\n","        # ================= Update D ================== # \n","        fake_B = G(real_A)\n","\n","        # discriminator loss \n","        disc_real = D(torch.cat((real_A, real_B), dim=1))\n","        disc_fake = D(torch.cat((real_A, fake_B.detach()), dim=1))\n","                              \n","        real_label = torch.ones_like(disc_real).cuda()\n","        fake_label = torch.zeros_like(disc_fake).cuda()\n","        loss_disc_real = BCE_criterion(disc_real, real_label)\n","        loss_disc_fake = BCE_criterion(disc_fake, fake_label)\n","        \n","        loss_D = loss_disc_real + loss_disc_fake\n","        \n","        # back prop.\n","        optimizer_D.zero_grad()\n","        loss_D.backward()\n","        optimizer_D.step()\n","\n","        # ================= Update G ================== # \n","        # generator loss\n","        disc_fake = D(torch.cat((real_A, fake_B), dim=1))\n","\n","        real_label = torch.ones_like(disc_fake).cuda()\n","        loss_disc_fake = BCE_criterion(disc_fake, real_label)\n","        loss_L1 = nn.L1Loss()(fake_B, real_B)\n","        loss_G = loss_disc_fake + LAMBDA_L1 * loss_L1\n","\n","        # back prop.\n","        optimizer_G.zero_grad()\n","        loss_G.backward()\n","        optimizer_G.step()\n","\n","        loss_D_total += loss_D.item()\n","        loss_G_total += loss_G.item()\n","        \n","        loss_G_sub_total += loss_G.item()\n","        loss_D_sub_total += loss_D.item()\n","\n","        # print current states\n","        print_freq = 10\n","        if batch_idx % print_freq  == 0 and batch_idx>0:\n","            print('Epoch : {} || {}/{} || loss_G={:.3f} loss_D={:.3f} time={:.3f} secs/iter'.format(\n","                epoch, batch_idx, len(train_loader), \n","                loss_G_sub_total/print_freq, loss_D_sub_total/print_freq, \n","                time.time()-t1\n","            ))\n","            # ================= Genearte example samples ================== # \n","            for bi, (real_A_test, real_B_test) in enumerate(test_loader):\n","                if bi>0:\n","                    break\n","                real_A_test = real_A_test[:NUM_EXAMPLES].cuda()\n","                real_B_test = real_B_test[:NUM_EXAMPLES].cuda()\n","                with torch.no_grad():\n","                    fake_B_test = G(real_A_test)\n","                    fake_B_test = fake_B_test[:NUM_EXAMPLES]\n","\n","                real_A_samples = (real_A_test + 1)/2\n","                real_B_samples = (real_B_test + 1)/2\n","                fake_B_samples = (fake_B_test + 1)/2\n","                examples = torch.cat((real_A_samples, fake_B_samples, real_B_samples), dim=0)\n","                examples = make_grid(examples, nrow=NUM_EXAMPLES)\n","                \n","                # log images\n","                logger.add_image('Generated_pairs', examples, iterations)\n","\n","            iterations += 1\n","            loss_G_sub_total = 0\n","            loss_D_sub_total = 0\n","\n","    loss_G_total /= len(train_loader)\n","    loss_D_total /= len(train_loader)\n","    \n","    # logging on tensorboard\n","    logger.add_scalar('loss_G', loss_G_total, epoch)\n","    logger.add_scalar('loss_D', loss_D_total, epoch)\n","\n","    # print current states\n","    print('Epoch : {} has done. AVG loss : loss_G={:.3f} loss_D={:.3f}'.format(\n","        epoch, loss_G_total, loss_D_total\n","    ))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting tensorboardx\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\r\u001b[K     |█                               | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.18.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx) (49.2.0)\n","Installing collected packages: tensorboardx\n","Successfully installed tensorboardx-2.1\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-ccf6fa2ba294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# back prop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mloss_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED\nException raised from createCuDNNHandle at /pytorch/aten/src/ATen/cudnn/Handle.cpp:9 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7facc1fa01e2 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x100ca68 (0x7facc3415a68 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: at::native::getCudnnHandle() + 0x108d (0x7facc341734d in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: <unknown function> + 0xeda4cc (0x7facc32e34cc in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xed59ee (0x7facc32de9ee in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xed75db (0x7facc32e05db in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::cudnn_convolution_backward_input(c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0xb2 (0x7facc32e0b32 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xf3cd3b (0x7facc3345d3b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf6cb58 (0x7facc3375b58 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::cudnn_convolution_backward_input(c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0x1ad (0x7facfa1ec88d in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x223 (0x7facc32df203 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xf3ce25 (0x7facc3345e25 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xf6cbb4 (0x7facc3375bb4 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #13: at::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x1e2 (0x7facfa1fb242 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x2ec9c62 (0x7facfbebec62 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ede224 (0x7facfbed3224 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #16: at::cudnn_convolution_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x1e2 (0x7facfa1fb242 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::generated::CudnnConvolutionBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x258 (0x7facfbd45c38 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x3375bb7 (0x7facfc36abb7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7facfc366400 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7facfc366fa1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #21: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7facfc35f119 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fad09aff34a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #23: <unknown function> + 0xbd6df (0x7fad262e76df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #24: <unknown function> + 0x76db (0x7fad273c96db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #25: clone + 0x3f (0x7fad27702a3f in /lib/x86_64-linux-gnu/libc.so.6)\n"]}]},{"cell_type":"code","metadata":{"id":"EV838kqpQKBC","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597339900548,"user_tz":-540,"elapsed":60654,"user":{"displayName":"정휘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEvYY_jsUnL_sIQJUhiD9YojAd0EdopXXTZEId=s64","userId":"09080035346003251429"}}},"source":["# Check Tensorboard.\n","%ls runs\n","%load_ext tensorboard\n","%tensorboard --logdir runs --port 9999"],"execution_count":null,"outputs":[]}]}