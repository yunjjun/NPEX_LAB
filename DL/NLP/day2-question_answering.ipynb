{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIqz0_BWvDVq"
   },
   "source": [
    "# Install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gsm5y-MyvJqN"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pI-PZ6hzvDVs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go0FLX9byCy3"
   },
   "source": [
    "# Download SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N67jgNaivDVx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-23 00:59:17--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.110.153, 185.199.111.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30288272 (29M) [application/json]\n",
      "Saving to: ‘./squad_data/train-v1.1.json’\n",
      "\n",
      "train-v1.1.json     100%[===================>]  28.88M  13.5MB/s    in 2.1s    \n",
      "\n",
      "2020-08-23 00:59:20 (13.5 MB/s) - ‘./squad_data/train-v1.1.json’ saved [30288272/30288272]\n",
      "\n",
      "--2020-08-23 00:59:20--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4854279 (4.6M) [application/json]\n",
      "Saving to: ‘./squad_data/dev-v1.1.json’\n",
      "\n",
      "dev-v1.1.json       100%[===================>]   4.63M  6.99MB/s    in 0.7s    \n",
      "\n",
      "2020-08-23 00:59:21 (6.99 MB/s) - ‘./squad_data/dev-v1.1.json’ saved [4854279/4854279]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P ./squad_data https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "!wget -P ./squad_data https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrFD6acOygCB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAlEanIAyQaP"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r07fwOd2yHV3"
   },
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BebYUD4QvDV1"
   },
   "outputs": [],
   "source": [
    "data_dir = \"./squad_data\"\n",
    "train_path = os.path.join(data_dir, 'train-v1.1.json')\n",
    "dev_path = os.path.join(data_dir, 'dev-v1.1.json')\n",
    "with open(train_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(dev_path, 'r') as f:\n",
    "    dev_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dz4AYEmzvDV3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of data:  442\n",
      "\n",
      "Context example: \n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta\n",
      "\n",
      "QA example: \n",
      "answers [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}]\n",
      "question To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "id 5733be284776f41900661182\n"
     ]
    }
   ],
   "source": [
    "# train_data[\"data\"]\n",
    "print(\"Nb of data: \", len(train_data[\"data\"]))\n",
    "print()\n",
    "# print(train_data[\"data\"][0].keys())\n",
    "# print(len(train_data[\"data\"][0][\"paragraphs\"]))\n",
    "print(\"Context example: \")\n",
    "print(train_data[\"data\"][0][\"paragraphs\"][0][\"context\"][:200])\n",
    "print()\n",
    "print(\"QA example: \")\n",
    "for k, v in train_data[\"data\"][0][\"paragraphs\"][0][\"qas\"][0].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8jwNssg0U53"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKzLcVxivDV9"
   },
   "outputs": [],
   "source": [
    "from transformers.data.processors.squad import SquadProcessor, SquadV1Processor\n",
    "from transformers.data.processors.squad import squad_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EsBozQNvDWA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:03<00:00, 15.21it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = SquadV1Processor()\n",
    "examples = processor.get_dev_examples(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHJv10cevDWe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1320: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=256,\n",
    "            doc_stride=128, \n",
    "            max_query_length=64,\n",
    "            is_training=False,\n",
    "            return_dataset=\"pt\", \n",
    "            tqdm_enabled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZygFS3xhvDWg",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  101,  2029,  5088,  2136,  3421,  1996, 22309,  2012,  3565,  4605,\n",
      "         2753,  1029,   102,  3565,  4605,  2753,  2001,  2019,  2137,  2374,\n",
      "         2208,  2000,  5646,  1996,  3410,  1997,  1996,  2120,  2374,  2223,\n",
      "         1006,  5088,  1007,  2005,  1996,  2325,  2161,  1012,  1996,  2137,\n",
      "         2374,  3034,  1006, 10511,  1007,  3410,  7573, 14169,  3249,  1996,\n",
      "         2120,  2374,  3034,  1006, 22309,  1007,  3410,  3792, 12915,  2484,\n",
      "         1516,  2184,  2000,  7796,  2037,  2353,  3565,  4605,  2516,  1012,\n",
      "         1996,  2208,  2001,  2209,  2006,  2337,  1021,  1010,  2355,  1010,\n",
      "         2012, 11902,  1005,  1055,  3346,  1999,  1996,  2624,  3799,  3016,\n",
      "         2181,  2012,  4203, 10254,  1010,  2662,  1012,  2004,  2023,  2001,\n",
      "         1996, 12951,  3565,  4605,  1010,  1996,  2223, 13155,  1996,  1000,\n",
      "         3585,  5315,  1000,  2007,  2536,  2751,  1011, 11773, 11107,  1010,\n",
      "         2004,  2092,  2004,  8184, 28324,  2075,  1996,  4535,  1997, 10324,\n",
      "         2169,  3565,  4605,  2208,  2007,  3142, 16371, 28990,  2015,  1006,\n",
      "         2104,  2029,  1996,  2208,  2052,  2031,  2042,  2124,  2004,  1000,\n",
      "         3565,  4605,  1048,  1000,  1007,  1010,  2061,  2008,  1996,  8154,\n",
      "         2071, 14500,  3444,  1996,  5640, 16371, 28990,  2015,  2753,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(1), tensor(0), tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "len(dataset[1])\n",
    "print(dataset[1])\n",
    "# all_input_ids,\n",
    "# all_attention_masks,\n",
    "# all_token_type_ids,\n",
    "# all_start_positions,\n",
    "# all_end_positions,\n",
    "# all_cls_index,\n",
    "# all_p_mask,\n",
    "# all_is_impossible,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Z1-pZcP6hyf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qdaQzr0V6jXf"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMGHGYKgvDWk",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/23/2020 01:02:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/23/2020 01:02:46 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/jennybae/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/23/2020 01:02:46 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/23/2020 01:02:47 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/jennybae/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/23/2020 01:02:47 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/23/2020 01:02:47 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/jennybae/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/23/2020 01:02:48 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/jennybae/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "08/23/2020 01:02:50 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "08/23/2020 01:02:50 - WARNING - transformers.modeling_utils -   Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/23/2020 01:02:53 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=False, do_lower_case=False, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=256, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=10.0, output_dir='./outputs/squad', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=4, predict_file='squad_data/dev-v1.1.json', save_steps=4000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='squad_data/dev-v1.1.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/23/2020 01:02:53 - INFO - __main__ -   Creating features from dataset file at .\n",
      "100%|███████████████████████████████████████████| 48/48 [00:03<00:00, 15.02it/s]\n",
      "convert squad examples to features:   0%|             | 0/10570 [00:00<?, ?it/s]/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1320: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n",
      "convert squad examples to features:  36%|▎| 3777/10570 [00:23<00:46, 147.04it/s]^C\n",
      "Process ForkPoolWorker-1:\n",
      "convert squad examples to features:  36%|▎| 3808/10570 [00:24<00:42, 158.38it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/transformers/data/processors/squad.py\", line 112, in squad_convert_example_to_features\n",
      "    sub_tokens = tokenizer.tokenize(token)\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/transformers/tokenization_utils.py\", line 350, in tokenize\n",
      "    tokenized_text = split_on_tokens(no_split_token, text)\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/transformers/tokenization_utils.py\", line 334, in split_on_tokens\n",
      "    if sub_text not in self.unique_no_split_tokens:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/multiprocessing/pool.py\", line 720, in next\n",
      "    item = self._items.popleft()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"transformers/examples/question-answering/run_squad.py\", line 820, in <module>\n",
      "    main()\n",
      "  File \"transformers/examples/question-answering/run_squad.py\", line 762, in main\n",
      "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
      "  File \"transformers/examples/question-answering/run_squad.py\", line 456, in load_and_cache_examples\n",
      "    threads=args.threads,\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/transformers/data/processors/squad.py\", line 359, in squad_convert_examples_to_features\n",
      "    disable=not tqdm_enabled,\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/tqdm/std.py\", line 1130, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/multiprocessing/pool.py\", line 320, in <genexpr>\n",
      "    return (item for chunk in result for item in chunk)\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/multiprocessing/pool.py\", line 724, in next\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/home/jennybae/miniconda3/envs/npex/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# If you do not use colabs\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "!python transformers/examples/question-answering/run_squad.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --do_train \\\n",
    "    --train_file squad_data/dev-v1.1.json \\\n",
    "    --predict_file squad_data/dev-v1.1.json \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --doc_stride 128 \\\n",
    "    --output_dir ./outputs/squad \\\n",
    "    --per_gpu_train_batch_size=4   \\\n",
    "    --save_steps 4000 \\\n",
    "    --overwrite_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnNldNanvDWn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xqZ_kXi-Izj"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dymvlL52vDWp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/21/2020 14:43:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/21/2020 14:43:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/jennybae/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/21/2020 14:43:21 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/21/2020 14:43:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/jennybae/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/21/2020 14:43:21 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/21/2020 14:43:23 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/jennybae/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/21/2020 14:43:23 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/jennybae/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "08/21/2020 14:43:25 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "08/21/2020 14:43:25 - WARNING - transformers.modeling_utils -   Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/21/2020 14:43:28 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=False, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=256, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_best_size=5, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='./outputs/squad', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=8, predict_file='squad_data/dev-v1.1.json', save_steps=500, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
      "08/21/2020 14:43:28 - INFO - __main__ -   Loading checkpoint bert-base-uncased for evaluation\n",
      "08/21/2020 14:43:28 - INFO - __main__ -   Evaluate the following checkpoints: ['bert-base-uncased']\n",
      "08/21/2020 14:43:29 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/jennybae/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "08/21/2020 14:43:29 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/21/2020 14:43:29 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/jennybae/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "08/21/2020 14:43:32 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "08/21/2020 14:43:32 - WARNING - transformers.modeling_utils -   Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/21/2020 14:43:32 - INFO - __main__ -   Creating features from dataset file at .\n",
      "100%|███████████████████████████████████████████| 48/48 [00:03<00:00, 14.69it/s]\n",
      "convert squad examples to features:   0%|             | 0/10570 [00:00<?, ?it/s]/home/jennybae/miniconda3/envs/npex/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1320: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n",
      "convert squad examples to features: 100%|█| 10570/10570 [01:17<00:00, 137.27it/s\n",
      "add example index and unique id: 100%|█| 10570/10570 [00:00<00:00, 648467.73it/s\n",
      "08/21/2020 14:44:52 - INFO - __main__ -   Saving features into cached file ./cached_dev_bert-base-uncased_256\n",
      "08/21/2020 14:45:01 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "08/21/2020 14:45:01 - INFO - __main__ -     Num examples = 11717\n",
      "08/21/2020 14:45:01 - INFO - __main__ -     Batch size = 4\n",
      "Evaluating: 100%|███████████████████████████| 2930/2930 [01:03<00:00, 46.09it/s]\n",
      "08/21/2020 14:46:05 - INFO - __main__ -     Evaluation done in total 63.567387 secs (0.005425 sec per example)\n",
      "08/21/2020 14:46:05 - INFO - transformers.data.metrics.squad_metrics -   Writing predictions to: ./outputs/squad/predictions_.json\n",
      "08/21/2020 14:46:05 - INFO - transformers.data.metrics.squad_metrics -   Writing nbest to: ./outputs/squad/nbest_predictions_.json\n",
      "08/21/2020 14:46:14 - INFO - __main__ -   Results: {'exact': 0.05676442762535478, 'f1': 5.5555411204954535, 'total': 10570, 'HasAns_exact': 0.05676442762535478, 'HasAns_f1': 5.5555411204954535, 'HasAns_total': 10570, 'best_exact': 0.05676442762535478, 'best_exact_thresh': 0.0, 'best_f1': 5.5555411204954535, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/question-answering/run_squad.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --do_eval \\\n",
    "    --predict_file squad_data/dev-v1.1.json \\\n",
    "    --n_best_size 5 \\\n",
    "    --max_seq_length 256 \\\n",
    "    --doc_stride 128 \\\n",
    "    --output_dir ./outputs/squad \\\n",
    "    --per_gpu_eval_batch_size=4  \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOwTn6XP-YbX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXDJJVk8-Z_9"
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlBA6vkqvDWs"
   },
   "outputs": [],
   "source": [
    "output_dir = \"./outputs/squad\"\n",
    "pred_path = os.path.join(output_dir, 'predictions_.json')\n",
    "nbest_pred_path = os.path.join(output_dir, 'nbest_predictions_.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5XB3re5JvDWu"
   },
   "outputs": [],
   "source": [
    "with open(pred_path, \"r\") as pred_json:\n",
    "    pred = json.load(pred_json)\n",
    "with open(nbest_pred_path, \"r\") as nbest_pred_json:\n",
    "    nbest_pred = json.load(nbest_pred_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8n6jfbP6vDWx"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "squad_path = \"./squad_data\"\n",
    "file_path = os.path.join(squad_path, 'dev-v1.1.json')\n",
    "with open(file_path, 'r') as f:\n",
    "    dev_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM\n",
      "{'answers': [{'answer_start': 116, 'text': '2015'}, {'answer_start': 346, 'text': '2016'}, {'answer_start': 116, 'text': '2015'}], 'question': 'What year did the Denver Broncos secure a Super Bowl title for the third time?', 'id': '56bf10f43aeaaa14008c94fd'}\n",
      "\n",
      "PREDICT\n",
      "with various gold-themed initiatives, as well as\n",
      "\n",
      "N-BEST\n",
      "{'text': 'with various gold-themed initiatives, as well as', 'probability': 0.2824446786483579, 'start_logit': 0.6809132695198059, 'end_logit': 0.946496307849884}\n",
      "{'text': 'themed initiatives, as well as', 'probability': 0.25602599568125195, 'start_logit': 0.582709550857544, 'end_logit': 0.946496307849884}\n",
      "{'text': 'with various gold-themed initiatives, as well as temporarily suspending the tradition of naming', 'probability': 0.24208653932372287, 'start_logit': 0.6809132695198059, 'end_logit': 0.7923088669776917}\n",
      "{'text': 'themed initiatives, as well as temporarily suspending the tradition of naming', 'probability': 0.21944278634666725, 'start_logit': 0.582709550857544, 'end_logit': 0.7923088669776917}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# data_idx = random.randint(0,len(dev_data['data'])-1)\n",
    "data_idx =0\n",
    "paragraph_idx = random.randint(0, len(dev_data['data'][data_idx])-1)\n",
    "qas_idx = random.randint(0, len(dev_data['data'][data_idx]['paragraphs'][paragraph_idx][\"qas\"])-1)\n",
    "test_data_sample = dev_data['data'][data_idx]['paragraphs'][paragraph_idx][\"qas\"][qas_idx]\n",
    "idx = test_data_sample['id']\n",
    "\n",
    "print(\"PROBLEM\")\n",
    "print(test_data_sample)\n",
    "print()\n",
    "print(\"PREDICT\")\n",
    "print(pred[idx])\n",
    "print()\n",
    "print(\"N-BEST\")\n",
    "for ele in nbest_pred[idx]:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwKvqNVTvDW7"
   },
   "source": [
    "# Load fine-tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the input and convert to ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 70 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "how           2,129\n",
      "many          2,116\n",
      "parameters   11,709\n",
      "does          2,515\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "have          2,031\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "is            2,003\n",
      "really        2,428\n",
      "big           2,502\n",
      ".             1,012\n",
      ".             1,012\n",
      ".             1,012\n",
      "it            2,009\n",
      "has           2,038\n",
      "24            2,484\n",
      "-             1,011\n",
      "layers        9,014\n",
      "and           1,998\n",
      "an            2,019\n",
      "em            7,861\n",
      "##bed         8,270\n",
      "##ding        4,667\n",
      "size          2,946\n",
      "of            1,997\n",
      "1             1,015\n",
      ",             1,010\n",
      "02            6,185\n",
      "##4           2,549\n",
      ",             1,010\n",
      "for           2,005\n",
      "a             1,037\n",
      "total         2,561\n",
      "of            1,997\n",
      "340          16,029\n",
      "##m           2,213\n",
      "parameters   11,709\n",
      "!               999\n",
      "altogether   10,462\n",
      "it            2,009\n",
      "is            2,003\n",
      "1             1,015\n",
      ".             1,012\n",
      "34            4,090\n",
      "##gb         18,259\n",
      ",             1,010\n",
      "so            2,061\n",
      "expect        5,987\n",
      "it            2,009\n",
      "to            2,000\n",
      "take          2,202\n",
      "a             1,037\n",
      "couple        3,232\n",
      "minutes       2,781\n",
      "to            2,000\n",
      "download      8,816\n",
      "to            2,000\n",
      "your          2,115\n",
      "cola         15,270\n",
      "##b           2,497\n",
      "instance      6,013\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for token, id in zip(tokens, input_ids):    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make segment ids for the input (question:0, context:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340 ##m\"\n"
     ]
    }
   ],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340m\"\n"
     ]
    }
   ],
   "source": [
    "# Start with the first token.\n",
    "answer = tokens[answer_start]\n",
    "\n",
    "for i in range(answer_start + 1, answer_end + 1):\n",
    "    if tokens[i][0:2] == '##':\n",
    "        answer += tokens[i][2:]\n",
    "    else:\n",
    "        answer += ' ' + tokens[i]\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "day2_question-answering.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
